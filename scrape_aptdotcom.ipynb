{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMoXu+5PYYPXaCLcm60BtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/groovymarty/gracieslist/blob/main/scrape_aptdotcom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here is the apartments.com scraper!\n",
        "You can use this Colab notebook to execute the scraper as often as you want and accumulate results in a thing called a Pandas dataframe.  But when you close this notebook, the dataframe goes away.  So you need a way to save it.  One way that I've provided is to write it to a CSV file which you can then download.  See instructions below.  (Note the saved CSV file is also part of the Colab virtual machine and so it also goes away when you close the notebook.  So you need to download the CSV file to your computer to really save it.)\n",
        "\n",
        "TO DO:\n",
        "* Provide a way to save results to a Google sheet.\n",
        "* Provide a way to append results to a Google sheet or CSV file that you already have."
      ],
      "metadata": {
        "id": "DcvTn3sd_KYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute these code blocks once to set things up."
      ],
      "metadata": {
        "id": "9Mqq7pbYAS-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are the imports we need\n",
        "import requests\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas"
      ],
      "metadata": {
        "id": "2aVhvIIx_fNW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to build and send requests\n",
        "def build_url(where, page):\n",
        "  if page == 1:\n",
        "    return f\"https://www.apartments.com/{where}/\"\n",
        "  else:\n",
        "    return f\"https://www.apartments.com/{where}/{page}/\"\n",
        "\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:107.0) Gecko/20100101 Firefox/107.0\"}\n",
        "\n",
        "def send_request(url):\n",
        "  return requests.get(url, headers=headers).text"
      ],
      "metadata": {
        "id": "JBrifiSCAv32"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to process the HTML that comes back from a request\n",
        "# Return array of result rows\n",
        "def process_result(soup):\n",
        "  rows = []\n",
        "  placards = soup.find(id=\"placards\")\n",
        "  properties = placards.find_all(\"div\", class_=\"property-info\")\n",
        "  for prop in properties:\n",
        "    address_div = prop.find(\"div\", class_=\"property-address\")\n",
        "    beds_div = prop.find(\"div\", class_=\"bed-range\")\n",
        "    price_div = prop.find(\"div\", class_=\"price-range\")\n",
        "    if address_div and beds_div and price_div:\n",
        "      address = address_div.get_text()\n",
        "      beds = beds_div.get_text()\n",
        "      price = price_div.get_text()\n",
        "      link = prop.find(\"a\", class_=\"property-link\").get(\"href\")\n",
        "      print(f'found: \"{where}\",\"{address}\",\"{beds}\",\"{price}\"')\n",
        "      rows.append({\n",
        "          \"Where\": where,\n",
        "          \"Address\": address,\n",
        "          \"Beds\": beds,\n",
        "          \"Price\": price,\n",
        "          \"Link\": link\n",
        "      })\n",
        "  return rows\n"
      ],
      "metadata": {
        "id": "3K-cact6Ig1m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-level functions to drive the scraping process\n",
        "def random_delay():\n",
        "  time.sleep(3+random.random()*5)\n",
        "\n",
        "def scrape_all_pages(where):\n",
        "  rows = []\n",
        "  page = 1\n",
        "  while True:\n",
        "    print(\"delaying...\")\n",
        "    random_delay()\n",
        "    print(f\"getting {where} page {page}\")\n",
        "    # send request to site and get result\n",
        "    html_text = send_request(build_url(where, page))\n",
        "    # parse and process result\n",
        "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
        "    rows.extend(process_result(soup))\n",
        "    # pagination logic\n",
        "    page_range = soup.find(\"span\", class_=\"pageRange\")\n",
        "    if page_range:\n",
        "        last_page = int(page_range.get_text().split()[-1])\n",
        "    else:\n",
        "        last_page = 1\n",
        "    if page >= last_page:\n",
        "        break\n",
        "    else:\n",
        "        page += 1\n",
        "  return rows"
      ],
      "metadata": {
        "id": "6G2AOw_hHskF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Below are the parameters.  Edit them as you wish.\n",
        "You must execute this code block at least once, and again when you change any of the the parameter values."
      ],
      "metadata": {
        "id": "YGoBgk9e_rCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "places = []\n",
        "places.append(\"wabasha-county-mn\")\n",
        "places.append(\"rochester-mn\")"
      ],
      "metadata": {
        "id": "XGh7Su4gAA1l"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This code block creates an empty dataframe to accumulate the results.\n",
        "You must execute this code block at least once.  Run it again if you want to clear the results and start over."
      ],
      "metadata": {
        "id": "PusTipjmQEWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pandas.DataFrame(columns=[\"Where\", \"Address\", \"Beds\", \"Price\", \"Link\"])"
      ],
      "metadata": {
        "id": "E4cpj2ieQRZQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute the following code block to scrape the site.\n",
        "It will fully scrape the site according to your parameters, logging messages to show its progress, and adding result rows to the dataframe.  Run as often as you like."
      ],
      "metadata": {
        "id": "iAKqQePWA5eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for where in places:\n",
        "  rows = scrape_all_pages(where)\n",
        "  df = df.append(rows)\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "cQvY38_SC6bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute the following block to view the result dataframe."
      ],
      "metadata": {
        "id": "5VmNdUOzQ881"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "IaidU8yRL4NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute the following block to save the result dataframe to a CSV file\n",
        "Edit the file name as you wish.  Use the file explorer to the left to find the CSV file.  (Look in the \"content\" folder.)  Then you can download the file if desired.  Note the files in the \"content\" folder will go away when you close the Colab notebook."
      ],
      "metadata": {
        "id": "z7hCV93IRnlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"results.csv\")"
      ],
      "metadata": {
        "id": "GcoiATnBRwR1"
      },
      "execution_count": 48,
      "outputs": []
    }
  ]
}